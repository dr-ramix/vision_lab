1. Datensätze herunterladen

Im Terminal in diesem Verzeichnis sein:
~/vision_lab/vision_lab/main/scripts

Job abschicken mit diesem Befehl:
sbatch download_sources.slurm

Erwartete Ausgabe:
Submitted batch job <job-ID>

Job-Status überprüfen mit diesem Befehl:
squeue -u $USER

-> Beispiel Ausgabe:
JOBID    PARTITION   NAME               ST   TIME           NODES NODELIST
123456   NvidiaAll   download_sources   R    00:02:31 1     node42

Status (ST):
PD → pending (wartet)
R → running (läuft)
CG → finishing
weg → fertig (nicht mehr in Queue)

Sobald ST = CG oder weg, weiter zu Schritt 2.



2. images_raw erzeugen

Nun werden alle Datensätze in eine einheitliche Projektstruktur überführt.

Schritte (analog zu 1):

cd ~/vision_lab/vision_lab/main/scripts
sbatch prepare_images_raw.slurm

Sobald ST = CG oder weg, weiter zu Schritt 3.


Was das Script macht:

- erkennt automatisch, ob ein Datensatz train/test besitzt
- entnimmt 10 % von train als val, falls val fehlt
- kopiert nur die Emotionen: anger, disgust, fear, happiness, sadness, surprise
- füllt den Ordner images_raw:

images_raw/
  train/
    <emotion>/
  val/
    <emotion>/
  test/
    <emotion>/

images_raw ist die Datenbasis für alle weiteren Schritte.



3. MTCNN + Cropping + Normalisierung ausführen

Jetzt wird das eigentliche Gesichts-Pre-Processing durchgeführt.

Schritte (analog zu 1 & 2):
cd ~/vision_lab/vision_lab/main/scripts
sbatch run_mtcnn_cropped.slurm


Was dieses Script macht:

- iteriert über images_raw/train|val|test/<emotion>
- führt aus:
  ~ MTCNN Gesichtserkennung
  ~ Augenbasierte Rotation
  ~ Cropping
  ~ Resize auf 64×64
  ~ Z-Normalisierung
  ~ überschreibt vorhandene Ergebnisse bewusst, falls sie existieren
- Ergebnis:

images_mtcnn_cropped/npy
  train/
    <emotion>/
  val/
    <emotion>/
  test/
    <emotion>/

images_mtcnn_cropped/png
  train/
    <emotion>/
  val/
    <emotion>/
  test/
    <emotion>/






